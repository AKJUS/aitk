{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is it like to be a robot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Philosopher Thomas Nagel famously asked \"What is it like to be a bat?\" in his 1974 paper of the same name.  Nagel argued that one might try to imagine what being a bat would be like, but without inhabiting its body and experiencing the world as it does through echolocation, we can't *really* know what it is like to be a bat.  \n",
    "\n",
    "German biologist Jakob Von Uexk&uuml;ll developed the term *umwelt* to capture the idea of how the world is experienced by a particular organism.  When translated from German this equates to \"self-centered world\". For a dog the world is dominated by smell, whereas for most humans the world is experienced primarily through vision. \n",
    "\n",
    "Here we will explore a robot's umwelt, and try to imagine what is it like to be a robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.26'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aitk.robots import World, Scribbler, RangeSensor, Camera, __version__\n",
    "__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a world\n",
    "\n",
    "Let's create a world with several uniquely colored rooms along a long corridor for our robot to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to: 9917436\n"
     ]
    }
   ],
   "source": [
    "world = World(width=300, height=200)\n",
    "world.add_wall(\"orange\",50,75,150,85)\n",
    "world.add_wall(\"yellow\",150,75,250,85)\n",
    "world.add_wall(\"orange\",145,0,150,75)\n",
    "world.add_wall(\"yellow\",150,0,155,75)\n",
    "world.add_wall(\"red\",0,125,165,135)\n",
    "world.add_wall(\"red\",220,125,225,200)\n",
    "world.add_wall(\"blue\",225,125,230,200)\n",
    "world.add_wall(\"pink\",155,0,185,30)\n",
    "robot = Scribbler(x=30, y=30, a=-100, max_trace_length=600)\n",
    "robot.add_device(RangeSensor(position=(6,-6),width=57.3,max=20,a=0,name=\"left-ir\"))\n",
    "robot.add_device(RangeSensor(position=(6,6),width=57.3,max=20,a=0,name=\"right-ir\"))\n",
    "robot.add_device(Camera(width=64,height=32))\n",
    "world.add_robot(robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the world\n",
    "\n",
    "Let's watch the world from a bird's-eye view as the robot moves around.  This gives us a *distal* perspective on the world.  We are not experiencing the world as the robot does, but instead have a top-down global view of what is happening. \n",
    "\n",
    "Notice that there is a pink box located in the yellow room. Later we will be trying to find this box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ecff6d237e4342a0e9d65364490eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x08\\x06\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "world.watch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the robot's view\n",
    "\n",
    "At the same time let's watch how the robot is experiencing the world through it's camera.  This gives us a *proximal* perspective on the world, from the agent's point of view.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2404ea371c2e46aa9584e77200a7f332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<style>img.pixelated {image-rendering: pixelated;}</style>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e16c1225b1c44958e7a424e58211e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<style>img.pixelated {image-rendering: pixelated;}</style>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df1d398e6c74033aa68d889a8072824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<style>img.pixelated {image-rendering: pixelated;}</style>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161686d17ca144b893856faf0d79f32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00@\\x00\\x00\\x00 \\x08\\x06\\x00\\x00\\x00\\xa2\\x9d~\\x84\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "robot[\"camera\"].watch(width=\"500px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating through the world\n",
    "\n",
    "Below is a simple controller that tries to keep moving forward while avoiding any obstacles that it encounters.  It is only using the robot's range sensors to make navigation decisions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.state[\"timer\"] = 0\n",
    "\n",
    "def avoid(robot):\n",
    "    left = robot[0].get_distance()\n",
    "    right = robot[1].get_distance()\n",
    "    if left == robot[0].get_max() and right == robot[1].get_max() and \\\n",
    "        robot.state[\"timer\"] == 0:\n",
    "        robot.move(0.5, 0)\n",
    "    elif robot.state[\"timer\"] > 0 and robot.state[\"timer\"] < 5:\n",
    "        robot.state[\"timer\"] += 1\n",
    "    elif left < robot[0].get_max():\n",
    "        robot.move(0.1, -0.3)\n",
    "        robot.state[\"timer\"] = 1\n",
    "    elif right < robot[1].get_max():\n",
    "        robot.move(0.1, 0.3)\n",
    "        robot.state[\"timer\"] = 1\n",
    "    else:\n",
    "        robot.state[\"timer\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe both distal and proximal perspectives\n",
    "\n",
    "Now let's watch the world from both the global, top-down view and the local, robot-based view at the same time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed: 9917436\n",
      "Simulation stopped at: 00:00:10.0; speed 0.98 x real time\n"
     ]
    }
   ],
   "source": [
    "world.reset()\n",
    "world.seconds(10, [avoid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience the world as the robot does\n",
    "\n",
    "Seeing the world through the robot's camera is very different then seeing it from the bird's-eye view.  Let's try to really take the robot's perspective. Hide the top-down view of the world. Now you will try to control the robot using only the robot's sensors to guide you. You goal is to traverse the hallway to the the yellow room and approach the pink box there.  \n",
    "\n",
    "We will create a dashboard where you can see all the robot's sensor readings and you can control the robot's movements via a joystick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This world is not running\n"
     ]
    }
   ],
   "source": [
    "from aitk.utils import make_joystick\n",
    "joystick = make_joystick(scale=[.4, .4], function=robot.move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7435ee9248a046fb9c006ca0136685e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<style>img.pixelated {image-rendering: pixelated;}</style>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82fe65ca2cc54842bfea7717b77b5900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<style>img.pixelated {image-rendering: pixelated;}</style>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2377a5bd3d14c369f64406826b37137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Canvas(height=250, layout=Layout(max_height='250px', min_width='250px'), width=2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import HBox, VBox, Layout\n",
    "layout = Layout(width=\"700px\")\n",
    "VBox(children=[\n",
    "    HBox(children=[\n",
    "        joystick.get_widget(), \n",
    "        robot[\"camera\"].get_widget(),\n",
    "    ]),\n",
    "    VBox(children=[\n",
    "        robot.get_widget(show_robot=False, attributes=[\"stalled\"]),\n",
    "        robot[\"left-ir\"].get_widget(title=\"Left IR\", attributes=[\"reading\"]),\n",
    "        robot[\"right-ir\"].get_widget(title=\"Right IR\", attributes=[\"reading\"]),\n",
    "    ])\n",
    " ], layout=layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below when you are ready to start controlling the robot via the dashboard. Use your mouse to push the joystick in different directions. The \"Stalled\" sensor is True when the robot is stuck and unable to move in the current direction.  If this happens try reversing the direction of movement. The \"IR\" sensors detect obstacles on the robot's left and right.  The smaller the value the closer the obstacle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting world.run() in background. Use world.stop()\n"
     ]
    }
   ],
   "source": [
    "world.run(background=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below when you want to stop using the dashboard.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping thread...\n"
     ]
    }
   ],
   "source": [
    "world.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back to the top-down world view to see how you did. Did you crash into any walls?  Did traversing the hallway take longer than you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try again\n",
    "\n",
    "At the bottom of the watched world you can see the time it took for you to reach the pink box.  Start again from the top of this notebook and see if you can reach the pink box faster this time.  By practicing, you should get better at seeing the world from the robot's perspective.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Some Cognitive Scientists believe that a key to understanding cognition is embracing the fact that organisms are embedded in environments.  Brains evolved to control bodies, and it is the interplay between the brain, the body, and the environment from which cognition emerges. Hopefully this experience of taking the robot's proximal perspective gave you a taste of what it is like to be a robot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. https://en.wikipedia.org/wiki/What_Is_It_Like_to_Be_a_Bat\n",
    "2. https://en.wikipedia.org/wiki/Umwelt\n",
    "3. https://en.wikipedia.org/wiki/Embodied_embedded_cognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
