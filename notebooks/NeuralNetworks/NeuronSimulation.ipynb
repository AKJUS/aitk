{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {}
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ArtificialIntelligenceToolkit/aitk/blob/master/notebooks/NeuralNetworks/NeuronSimulation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "-3XVnVeEX4pH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation of an artificial neuron\n",
        "\n",
        "Deep neural networks consist of multiple layers, each made up of a collection of neurons. Every neuron calculates its activation—essentially its level of activity—based on the weighted connections it has with other neurons in the network. This activation reflects how the neuron responds to a given input.\n",
        "\n",
        "The process each neuron follows is fairly simple. It starts by computing a weighted sum of the activations from all the neurons it's connected to—this is known as the net input. Then, it applies an activation function to this net input. There are several types of activation functions, and the one used affects the neuron's output. This resulting activation is then sent to the neurons in the next layer of the network.\n",
        "\n",
        "In this notebook, you'll have the opportunity to explore how a single neuron works. You can experiment with different incoming activations, weights, and activation functions to see how they influence the neuron's output.\n",
        "\n",
        "The following code was developed in collaboration with ChatGPT."
      ],
      "metadata": {
        "id": "IIfwUuYg-yad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import FloatSlider, Dropdown, interact\n",
        "from matplotlib.patches import Circle, FancyBboxPatch\n",
        "from matplotlib import cm\n",
        "\n",
        "# Activation functions and their names\n",
        "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
        "def tanh(x): return np.tanh(x)\n",
        "def relu(x): return np.maximum(0, x)\n",
        "\n",
        "activation_funcs = {\n",
        "    \"Sigmoid\": (sigmoid, \"Sigmoid Activation\"),\n",
        "    \"Tanh\": (tanh, \"Tanh Activation\"),\n",
        "    \"ReLU\": (relu, \"ReLU Activation\")\n",
        "}\n",
        "\n",
        "# Helper to draw input arrows\n",
        "def draw_arrow_to_neuron(ax, start, end, label, x_val, w_val):\n",
        "    dx = end[0] - start[0]\n",
        "    dy = end[1] - start[1]\n",
        "    ax.arrow(start[0], start[1], dx, dy, head_width=0.04, head_length=0.08, fc='black', ec='black')\n",
        "    ax.text(start[0] - 0.07, start[1], f\"{label}={x_val:.1f}\", fontsize=9, va='center', ha='right')\n",
        "    ax.text(start[0] + dx/2 - 0.4, start[1] + dy/2, f\"w={w_val:.2f}\", fontsize=10, color='blue')\n",
        "\n",
        "# Main interactive neuron visualization\n",
        "def draw_interactive_neuron(x1, x2, wbias, w1, w2, activation_name):\n",
        "    x_bias = 1.0\n",
        "    inputs = np.array([x1, x2, x_bias])\n",
        "    weights = np.array([w1, w2, wbias])\n",
        "    z = np.dot(inputs, weights)\n",
        "\n",
        "    activation_fn, activation_title = activation_funcs[activation_name]\n",
        "    output = activation_fn(z)\n",
        "\n",
        "    # Normalize output for neuron size and color\n",
        "    norm_output = (output + 1) / 2 if activation_name == \"Tanh\" else output\n",
        "    norm_output = np.clip(norm_output, 0, 1)\n",
        "    color = cm.viridis(norm_output)\n",
        "    radius = 0.25 + 0.25 * norm_output\n",
        "\n",
        "    # Set up full figure with activation subplot\n",
        "    fig = plt.figure(figsize=(8, 5))\n",
        "    grid = fig.add_gridspec(1, 2, width_ratios=[2.5, 1])\n",
        "\n",
        "    ax = fig.add_subplot(grid[0])\n",
        "    ax.set_xlim(-1.8, 3)\n",
        "    ax.set_ylim(-1.5, 1.5)\n",
        "    ax.axis('off')\n",
        "\n",
        "    neuron_center = (1.6, 0)\n",
        "    net_input_box_center = (0.3, 0)\n",
        "\n",
        "    # Draw arrows from inputs\n",
        "    input_coords = [(-1.3, 1), (-1.3, -1), (-1.3, 0)]\n",
        "    input_labels = ['x1', 'x2', 'bias']\n",
        "    input_values = [x1, x2, 1.0]\n",
        "    weight_values = [w1, w2, wbias]\n",
        "\n",
        "    for start, label, x_val, w_val in zip(input_coords, input_labels, input_values, weight_values):\n",
        "        draw_arrow_to_neuron(ax, start, net_input_box_center, label, x_val, w_val)\n",
        "\n",
        "    # Net input box\n",
        "    box_width, box_height = 0.7, 0.45\n",
        "    box = FancyBboxPatch((net_input_box_center[0] - box_width / 2, net_input_box_center[1] - box_height / 2),\n",
        "                         width=box_width, height=box_height,\n",
        "                         boxstyle=\"round,pad=0.05\", linewidth=1, edgecolor='black', facecolor='lightblue')\n",
        "    ax.add_patch(box)\n",
        "    ax.text(*net_input_box_center, f\"z = {z:.2f}\", ha='center', va='center', fontsize=10, weight='bold')\n",
        "\n",
        "    # Arrow from box to neuron\n",
        "    ax.arrow(net_input_box_center[0] + box_width / 2, net_input_box_center[1],\n",
        "             neuron_center[0] - net_input_box_center[0] - box_width / 2, 0,\n",
        "             head_width=0.04, head_length=0.08, fc='black', ec='black')\n",
        "\n",
        "    # Neuron circle\n",
        "    neuron = Circle(neuron_center, radius=radius, color=color, ec='black')\n",
        "    ax.add_patch(neuron)\n",
        "    ax.text(*neuron_center, f\"{output:.2f}\", ha='center', va='center', fontsize=10, weight='bold', color='white')\n",
        "\n",
        "    # Output arrow\n",
        "    ax.arrow(neuron_center[0] + radius, neuron_center[1], 0.6, 0,\n",
        "             head_width=0.04, head_length=0.08, fc='green', ec='green')\n",
        "    ax.text(neuron_center[0] + radius + 0.7, 0.1, \"Output\", fontsize=9, color='green')\n",
        "\n",
        "    # Draw activation function graph\n",
        "    ax_act = fig.add_subplot(grid[1])\n",
        "    x_vals = np.linspace(-6, 6, 200)\n",
        "    y_vals = activation_fn(x_vals)\n",
        "    ax_act.plot(x_vals, y_vals, label=activation_name)\n",
        "    ax_act.axvline(z, color='red', linestyle='--', linewidth=1)\n",
        "    ax_act.plot(z, output, 'ro', label='Current Output')\n",
        "    ax_act.set_title(activation_title, fontsize=11)\n",
        "    ax_act.set_xlabel(\"z\")\n",
        "    ax_act.set_ylabel(\"activation(z)\")\n",
        "    ax_act.grid(True)\n",
        "\n",
        "    # Adjust y-axis limits by activation type\n",
        "    if activation_name == \"Tanh\":\n",
        "        ax_act.set_ylim(-1.1, 1.1)\n",
        "    else:\n",
        "        ax_act.set_ylim(-0.1, 1.1)\n",
        "\n",
        "    ax_act.legend(fontsize=8)\n",
        "\n",
        "    plt.suptitle(f\"Activation: {activation_name} | Net input z: {z:.2f} | Output: {output:.3f}\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Interactive UI\n",
        "interact(\n",
        "    draw_interactive_neuron,\n",
        "    x1=FloatSlider(value=1.0, min=-5, max=5, step=0.1, description=\"x1\"),\n",
        "    x2=FloatSlider(value=1.0, min=-5, max=5, step=0.1, description=\"x2\"),\n",
        "    wbias=FloatSlider(value=0.0, min=-5, max=5, step=0.1, description=\"w_bias\"),\n",
        "    w1=FloatSlider(value=0.5, min=-5, max=5, step=0.1, description=\"w1\"),\n",
        "    w2=FloatSlider(value=0.5, min=-5, max=5, step=0.1, description=\"w2\"),\n",
        "    activation_name=Dropdown(options=[\"Sigmoid\", \"Tanh\", \"ReLU\"], value=\"Sigmoid\", description=\"Activation\")\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791,
          "referenced_widgets": [
            "c429444a8f564e9cbc1d67b8cc3adbcb",
            "3fc04c8387ac421caef3139a00404b09",
            "8d72519412ab4e15bfe46c0bcaba800c",
            "d03583182ec04c1296d15d4e47a305be",
            "b2bacd7fb61246b38fcbbb942300cc77",
            "132278b658634709bb06a547c571ab9d",
            "22f695c08c0e48309b347f0ce5dc57c9",
            "3f66347bbd384cf3b003d1a94909cda6",
            "65d4c8ec21d247549dc22827fa514f0f",
            "1c3a319d6a544965a4a08a0b10c4cddd",
            "b269fac58e6242d88530d3a8791ef373",
            "75f456d066cc4490982a3304c44b714a",
            "b9197ea1b53c40bdb1612564a93ab3ee",
            "ca8fd5e53ef54180ae2932bf0e355596",
            "bdab30e944774adcabab58bdcd33d48a",
            "a27ea19f57184b33ad68e63bc65c5db6",
            "7757c8330e9a448791388094fb4794d2",
            "b55a1e7b80ed48e6bb6cf53e2135ffee",
            "f235925dfb6c47cab3b0e753b12568c5",
            "d16bccc135194b2cb269dc58647dbe2c",
            "48f46cb15e274080b992affd05399fb8",
            "e0be4ffc8e114634a5049ac1d06e8750"
          ]
        },
        "id": "1Q5sW1la-Tb0",
        "outputId": "ccc9fa01-e238-4750-8c67-8bc9a07519c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "interactive(children=(FloatSlider(value=1.0, description='x1', max=5.0, min=-5.0), FloatSlider(value=1.0, desc…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c429444a8f564e9cbc1d67b8cc3adbcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.draw_interactive_neuron(x1, x2, wbias, w1, w2, activation_name)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>draw_interactive_neuron</b><br/>def draw_interactive_neuron(x1, x2, wbias, w1, w2, activation_name)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/tmp/ipython-input-1-3824772864.py</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}
